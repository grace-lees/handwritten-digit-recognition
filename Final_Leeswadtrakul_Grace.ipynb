{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Leeswadtrakul_Grace.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grace-lees/handwritten-digit-recognition/blob/main/Final_Leeswadtrakul_Grace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLZ91mmvMQyl"
      },
      "source": [
        "## **Machine Learning Final Project DUE: Friday May 7th 11:59pm**\n",
        "\n",
        "**Note: Please read all the instructions carefully before starting the project.**\n",
        "\n",
        "For your final project you will build an ML model to analyze a dataset of your choice. You are welcome to keep working on the data in your EDA project if your data is large enough (at least 1000 rows for simple models and at least 10,000 for more complex models) or you can choose from the datasets/project suggestions below.\n",
        "\n",
        "In this project make sure that you:\n",
        "- Have a large enough dataset\n",
        "- Split your data in training and testing\n",
        "- Explore your data to inform which type of model to choose (no need if you are using your EDA dataset)\n",
        "- Try different models on your training dataset - then select the most promising model\n",
        "- Use cross validation to fine tune the model’s parameters such as alpha in lasso\n",
        "- Simplify your model using regularization, prunnning, drop-out, etc. to avoid overfitting\n",
        "- Communicate your model’s performance and make sure you compare it to a benchmark when appropriate\n",
        "- Plot interesting graphs and results\n",
        "- Write and publish your article to medium\n",
        "- Commit your code to your GitHub\n",
        "\n",
        "Please ensure you handle all the preprocessing before the modeling.\n",
        "\n",
        "Suggestions for project:\n",
        "You can take a look at the resources given below for choosing a dataset for your project. \n",
        "\n",
        "- Traffic sign detection - https://benchmark.ini.rub.de/gtsdb_dataset.html\n",
        "- Cat and dog classifier - https://www.kaggle.com/c/dogs-vs-cats/data\n",
        "- Other datasets from Kaggle - https://www.kaggle.com/data/41592"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mirWm3dBPv90"
      },
      "source": [
        "## **Grading Criteria**\n",
        "\n",
        "- Show clear exploration of the data to justify model choice\n",
        "- Train mutliple models and clearly articulate why you chose your final model\n",
        "- Show your performance on test dataset\n",
        "- Clear and concise write-up with clear well-documented figures\n",
        "- Commit your code to GitHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZRNqxFcP4wx"
      },
      "source": [
        "## **Submission Details**\n",
        "\n",
        "This is an individual assignment. You may not work in groups. The assignment is due on Friday (05/07/2021)\n",
        "- To submit your assignment, download your notebook and the dataset, zip the dataset and notebook, and submit the zipped file on blackboard.\n",
        "- Make sure the notebook is named in the format - Final_LastName_FirstName. If you are submitting a zipped file, please name the file as well in the same format.\n",
        "- Please include the link to access your blog and your github repo in your notebook.\n",
        "- Also include the link to your notebook, github repo and the blog in the submission on blackboard. Please ensure the TAs have the required access to your notebooks and the github repo.\n",
        "\n",
        "**Note - If the dataset is too large to be zipped and submitted on blackboard, only submit your notebook, add your dataset to your google drive and share a link to the file in your notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0LfrsrDvptu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_71ZMw0vrMf"
      },
      "source": [
        "Github repo - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOkecWFoS3dB"
      },
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx9XAAhtTDG_"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "import matplotlib.pyplot as plot\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfy7bH5ATEMD"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Fbdwq4TUMb"
      },
      "source": [
        "# read data from kaggle\n",
        "df = pd.read_csv('train.csv')\n",
        "df1 = pd.read_csv('test.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOLgPqZCpa-I"
      },
      "source": [
        "#test train split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('label',axis=1), df['label'], test_size=0.2, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0gLcjsBpeL0",
        "outputId": "8d3a006c-bc2a-4a25-c67d-e075e2eeec9f"
      },
      "source": [
        "#data pre processing\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_train = np.array_split(X_train, len(X_train))\n",
        "for t in range(len(X_train)):\n",
        "    X_train[t] = np.reshape(X_train[t], (28,28))\n",
        "X_train = np.expand_dims(X_train,-1)\n",
        "np.shape(X_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18817, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "773CGevmpfcj",
        "outputId": "4e03118c-1fda-455a-9583-ae526270cc74"
      },
      "source": [
        "X_test = np.array(X_test)\n",
        "X_test = np.array_split(X_test, len(X_test))\n",
        "for t in range(len(X_test)):\n",
        "    X_test[t] = np.reshape(X_test[t], (28,28))\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "np.shape(X_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4705, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDlMH-W5phqp"
      },
      "source": [
        "#data augmentation\n",
        "\n",
        "tgen = ImageDataGenerator( rescale=1./255,\n",
        "      rotation_range=10,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.01,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=False,\n",
        "      fill_mode='nearest'\n",
        "                                  )\n",
        "vgen = ImageDataGenerator( rescale=1./255,\n",
        "      rotation_range=30,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.01,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=False,\n",
        "      fill_mode='nearest'\n",
        "                                  )\n",
        "\n",
        "batchSize = 128\n",
        "train_generator = tgen.flow(X_train,y_train,batch_size=batchSize)\n",
        "\n",
        "val_generator = vgen.flow(X_test, y_test,batch_size=batchSize)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqNarJ0Bpk4E",
        "outputId": "3a7748aa-ca70-4520-fab0-309fe9543488"
      },
      "source": [
        "#model definition CNN model\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Conv2D(64,(3,3),input_shape = (28,28,1), activation = 'relu',padding = 'Same'),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Conv2D(64,(3,3), activation = 'relu',padding = 'Same'),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Conv2D(64,(3,3), activation = 'relu',padding = 'Same'),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Conv2D(64,(3,3), activation = 'relu',padding = 'Same'),\n",
        "tf.keras.layers.BatchNormalization(), \n",
        "tf.keras.layers.MaxPooling2D(2,2),\n",
        "tf.keras.layers.Dropout(.3),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Conv2D(64,(3,3), activation = 'relu',padding = 'Same'),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Conv2D(64,(3,3), activation = 'relu',padding = 'Same'),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Dropout(.3),\n",
        "tf.keras.layers.Conv2D(64,(3,3), activation = 'relu',padding = 'Same'),\n",
        "tf.keras.layers.BatchNormalization(), \n",
        "tf.keras.layers.MaxPooling2D(2,2),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Dropout(.3),\n",
        "tf.keras.layers.Conv2D(64,(3,3), activation = 'relu',padding = 'Same'),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Dropout(.3),\n",
        "tf.keras.layers.Flatten(),\n",
        "tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "tf.keras.layers.Dropout(.3),\n",
        "tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Dropout(.3),\n",
        "tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "    \n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 766,154\n",
            "Trainable params: 764,106\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnNnWQTupo9t"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if(logs.get('val_accuracy')>0.99):\n",
        "                print(\"Reached 99% val_accuracy so cancelling training!\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "\n",
        "callbacks = myCallback()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szOSWqC0ptyq"
      },
      "source": [
        "\n",
        "call_back = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, restore_best_weights=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZVN7ualpusu"
      },
      "source": [
        "lr_list = [0.001,0.01,0.1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_CimN2fpw3u",
        "outputId": "acb3c1c7-31a4-461a-a006-cbc316faddc5"
      },
      "source": [
        "#Cross validation\n",
        "\n",
        "for lr in lr_list:\n",
        "    ad  = tf.keras.optimizers.RMSprop(lr=lr, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "    model.compile(optimizer = ad, metrics = ['accuracy'], loss = 'sparse_categorical_crossentropy')\n",
        "    history = model.fit(train_generator,verbose=2,epochs =10,validation_data = val_generator, callbacks = [callbacks])\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "148/148 - 385s - loss: nan - accuracy: 0.5699 - val_loss: nan - val_accuracy: 0.0956\n",
            "Epoch 2/10\n",
            "148/148 - 380s - loss: nan - accuracy: 0.0967 - val_loss: nan - val_accuracy: 0.0956\n",
            "Epoch 3/10\n",
            "148/148 - 382s - loss: nan - accuracy: 0.0967 - val_loss: nan - val_accuracy: 0.0956\n",
            "Epoch 4/10\n",
            "148/148 - 381s - loss: nan - accuracy: 0.0967 - val_loss: nan - val_accuracy: 0.0956\n",
            "Epoch 5/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhpFj-L7pzHk"
      },
      "source": [
        "#we get best result for lr=0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0ah3ihjp76i"
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "import random \n",
        "k = np.array(X_train)\n",
        "n = random.randint(0,100)\n",
        "t = k[n]\n",
        "t = np.reshape(t,(28,28))\n",
        "#t = np.expand_dims(t,-1)\n",
        "g = plot.imshow(t,cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY6yfpGiqAdE"
      },
      "source": [
        "np.argmax(model.predict(np.expand_dims(X_train[n],0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1fAhGTdqDIS"
      },
      "source": [
        "from keras import models\n",
        "layers_outputs = [layer.output for layer in model.layers[:20]]\n",
        "activation_model = models.Model(inputs=model.input, outputs=layers_outputs) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arj4_ycBqE1e"
      },
      "source": [
        "X_train[9].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JFEqDe1qGnY"
      },
      "source": [
        "k=random.randint(0,100)\n",
        "img_tensor = X_train[k]\n",
        "img_tensor = np.expand_dims(img_tensor,0)\n",
        "activations = activation_model.predict(img_tensor) \n",
        "n =17\n",
        "t = np.reshape(X_train[k],(28,28))\n",
        "#fig, axs = plot.subplots(n+1,1)\n",
        "plot.matshow(t, cmap='gray')\n",
        "for h in range(n):\n",
        "    first_layer_activation = activations[h]\n",
        "    print(first_layer_activation.shape)\n",
        "    plot.matshow(first_layer_activation[0, :, :, 4], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXqObMoHqLu8"
      },
      "source": [
        "print(history.history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO7hw9btqMUc"
      },
      "source": [
        "g = sns.lineplot(history.epoch[:],history.history['accuracy'][:],label = 'accuracy')\n",
        "g.set(xlabel = 'epoc',ylabel  = 'acc')\n",
        "sns.lineplot(history.epoch[:],history.history['val_accuracy'][:], label = 'val_accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOwpZ9oUqRG6"
      },
      "source": [
        "#epocs vs loss graphs\n",
        "g = sns.lineplot(history.epoch[2:],history.history['loss'][2:],label = 'loss')\n",
        "g.set(xlabel = 'epoc',ylabel  = 'loss')\n",
        "sns.lineplot(history.epoch[2:],history.history['val_loss'][2:], label = 'val_loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n0qzXmkqSFR"
      },
      "source": [
        "df1 = df1/255.0\n",
        "X_test = df1\n",
        "X_test = np.array(X_test)\n",
        "X_test = np.array_split(X_test, len(X_test))\n",
        "for t in range(len(X_test)):\n",
        "    X_test[t] = np.reshape(X_test[t], (28,28))\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "\n",
        "k = model.predict(X_test)\n",
        "\n",
        "k = np.argmax(k,axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QdgSMvDqVL9"
      },
      "source": [
        "sample = pd.read_csv('sample_submission.csv')\n",
        "np.size(k)\n",
        "sample['Label'] = k\n",
        "sample.to_csv('sol.csv',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}